{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High to Low (높임말 -> 반말)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# © 2022 Joowhan Kim, Jaemu Heo, Jeonghui Kim <joy980721@gmail.com>\n",
    "# Made by MILab in Handong Global University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from hangul_utils import split_syllables, join_jamos\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "\n",
    "############ For Window ############ \n",
    "#from eunjeon import Mecab\n",
    "############ For Window ############ \n",
    "\n",
    "############ For Mac ############ \n",
    "from konlpy.tag import Mecab\n",
    "############ For Mac ############ \n",
    "\n",
    "#from hanspell import spell_checker\n",
    "from khaiii import KhaiiiApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List and Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Korean vowel combination\n",
    "con_dict = [\n",
    "    \n",
    "    ['ㅏㅣ','ㅐ'], ['ㅑㅣ','ㅒ'], ['ㅓㅣ','ㅔ'],\n",
    "    ['ㅕㅣ','ㅖ'], ['ㅗㅣ','ㅚ'], ['ㅗㅐ','ㅙ'],\n",
    "    ['ㅜㅓ','ㅝ'], ['ㅜㅔ','ㅞ'], ['ㅡㅣ','ㅢ'],\n",
    "    ['ㅣㅏ','ㅑ'], ['ㅣㅓ','ㅕ'], ['ㅣㅗ','ㅛ'],\n",
    "    ['ㅣㅜ','ㅠ'], ['ㅡㅓ','ㅓ'], ['ㅗㅏ','ㅘ'],\n",
    "    \n",
    "]\n",
    "\n",
    "#jongsung_list = [ 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "\n",
    "# lis_beta = ['EP+EF', 'VCP+EF', 'B+EF', 'B+EP+EF', 'B+VCP+EF', 'EF','EP']\n",
    "\n",
    "# lis_beta = ['EP+EF', 'EF', 'B+EF', 'B+EP+EF']\n",
    "\n",
    "#어말을 처리해 주기 위한 것으로, 나중에 EC등이 필요해 진다면 이 부분에 EC 등을 집어넣어준다. 참고로 이는 문장의 마지막에 위치해야한다.\n",
    "#특이 바로 밑의 이 부분은 형태소 태그가 이 리스트 안의 것과 일치하는 경우 단순 삭제를 하는 것\n",
    "lis_beta_ef = ['EP+EP+EF', 'EP+EF', 'EF', 'UNKNOWN']\n",
    "lis_beta_ef_h = ['EF', 'UNKNOWN']\n",
    "\n",
    "lis_tag_last = ['EF', 'UNKNOWN']\n",
    "\n",
    "# 존칭 명사 동사를 낮추기 위한 list\n",
    "lis_wk = [\n",
    "    \n",
    "    ['ㄱㅖ', 'ㅇㅣㅆㅇㅡ'], ['ㅈㅜㅁㅜ','ㅈㅏ'], ['ㅈㅏㅂㅅㅜ','ㅁㅓㄱㅇㅡ']\n",
    "    \n",
    "]\n",
    "# 존칭 종결어미\n",
    "lis_end = [\n",
    "    \n",
    "    'ㅅㅡㅂㄴㅣㄷㅏ', 'ㅅㅡㅂㄴㅣㄲㅏ',\n",
    "    'ㅂㄴㅣㄷㅏ', 'ㄴㅣㄷㅏ', 'ㅂㅅㅣㄷㅏ', 'ㅅㅣㄷㅏ', 'ㄹㄹㅐㅇㅛ','ㄹㅐㅇㅛ',\n",
    "    'ㅇㅡㅅㅔㅇㅛ', 'ㅅㅔㅇㅛ', 'ㄷㅔㅇㅛ', 'ㅇㅔㅇㅛ', 'ㅇㅖㅇㅛ', 'ㄴㅏㅇㅛ', 'ㅇㅡㄹㄲㅏㅇㅛ', 'ㅇㅣㄹㄲㅏㅇㅛ', 'ㄹㄲㅏㅇㅛ', 'ㅇㅡㄴㄱㅏㅇㅛ', 'ㅇㅣㄴㄱㅏㅇㅛ','ㄱㅜㄴㅇㅛ','ㄴㄱㅏㅇㅛ',\n",
    "    'ㄱㅗㅇㅛ','ㅇㅛ',\n",
    "    'ㅈㅛ',\n",
    "    'ㅅㅣㅂㅅㅣㅇㅗ', 'ㅅㅣㅇㅗ', 'ㅇㅗ',\n",
    "    'ㅂㄴㅣㄲㅏ',  \n",
    "    \n",
    "]\n",
    "\n",
    "# 반말 종결어미\n",
    "lis_end_2low = [\n",
    "    \n",
    "    'ㄷㅓㄹㅏ','ㄴㄷㅏ', 'ㅆㄷㅏ', 'ㄹㅗㄷㅏ', 'ㄷㅏ', 'ㄱㅔ', 'ㄴㅡㄴㄷㅏ',\n",
    "    'ㄹㅏ',\n",
    "    'ㅇㅑ', \n",
    "    'ㄴㅣㄲㅏ', 'ㄲㅏ', 'ㄹㄲㅏ', 'ㅈㅣ',\n",
    "    'ㄴㅣ', \n",
    "    'ㅇㅏ', 'ㅇㅓ',\n",
    "    'ㄷㅔ', 'ㄱㅏ','ㄹㅐ',\n",
    "    'ㅈㅏㄶㅇㅏ', 'ㄴㅔ','ㅇㅕ', 'ㄴㅏ','ㄱㅜㄴ','ㄱㅗ',\n",
    "    'ㅈㅣㅁㅏㄴ', 'ㅇㅡㄴㄷㅔ', 'ㅅㅓ', 'ㄷㅐ',\n",
    "    'ㄱㅓㄹ','ㄲㅔ', 'ㄴㅑ',\n",
    "]\n",
    "\n",
    "lis_ic = ['ㅇㅖ', 'ㄴㅔ', 'ㅇㅏㄴㅣㅇㅗ', 'ㅇㅏㄴㅣㅇㅛ']\n",
    "\n",
    "P_LIST = ['.', '?', '!', '\\'', '\\\"', 'ᆞ', 'ᆢ', 'ㆍ',  '”', '’',')', '(', ',', '”']\n",
    "\n",
    "SV_LIST = ['\\'', '\\\"', ':', ';']\n",
    "\n",
    "lis_plus = [\n",
    "    \n",
    "    'EP', 'VCP', \n",
    "    \n",
    "]\n",
    "############################## EF Dictionary ##############################\n",
    "###########################################################################\n",
    "\n",
    "############ 높임말 -> 반말 ############\n",
    "#현재 만들어진 것은 EF만 잘라낼 것이다. \n",
    "#원래 ef사전에 mapping 되는 것을 찾아낸다.\n",
    "#python dictionary로 접근\n",
    "#종결어미 처리\n",
    "EF = {\n",
    "    ###하십시오체###\n",
    "    #평서문\n",
    "    #'ㅂㄴㅣㄷㅏ': 'ㄷㅏ',\n",
    "    'ㅂㄴㅣㄷㅏ': 'special3',\n",
    "    'ㅅㅡㅂㄴㅣㄷㅏ':'special2',\n",
    "    'ㅇㅗㄹㅅㅣㄷㅏ':'ㄷㅏ', #**\n",
    "    'ㅂㅈㅣㅇㅛ':'지', #**\n",
    "    'ㅅㅣㅂㄴㅣㄷㅏ':'special1',\n",
    "    'ㅇㅡㅅㅣㅂㄴㅣㄷㅏ':'ㅇㅡㅅㅣㄴㄷㅏ',\n",
    "    'ㅇㅡㅅㅣㅂㄴㅣㄲㅏ':'ㅇㅡㅅㅣㄴㅣ',\n",
    "\n",
    "    #의문문\n",
    "    'ㅅㅡㅂㄴㅣㄲㅏ':'ㄴㅣ',\n",
    "    'ㅂㄴㅣㄲㅏ': 'ㄴㅣ',\n",
    "    'ㅅㅣㅂㄴㅣㄲㅏ':'special1', #EP+EF\n",
    "    #명령법\n",
    "    'ㅇㅡㅅㅔㅇㅛ': 'special0',\n",
    "    'ㅅㅔㅇㅛ':'special1',\n",
    "    'ㅅㅣㅇㅓㅇㅛ': 'special1',\n",
    "    'ㅅㅣㅂㅅㅣㅇㅗ':'ㅅㅣㅇㅗ',\n",
    "    #청유법\n",
    "    'ㅂㅅㅣㄷㅏ':'special4',\n",
    "    'ㅇㅡㅂㅅㅣㄷㅏ':'special4',\n",
    "    ###하오체###\n",
    "    \n",
    "    ###해요체###\n",
    "    #평서문\n",
    "    'ㅇㅓㅇㅛ':'ㅇㅓ',\n",
    "    'ㅇㅏㅇㅛ':'ㅇㅏ',\n",
    "    'ㅈㅛ':'ㅈㅣ',\n",
    "    'ㅇㅔㅇㅛ':'ㅇㅑ',\n",
    "    'ㅇㅖㅇㅛ':'ㅇㅑ',\n",
    "    'ㅇㅛ':'special5',\n",
    "    'ㄷㅐㅇㅛ':'ㄷㅐ',\n",
    "    'ㄷㅔㅇㅛ':'ㄷㅔ',\n",
    "    'ㄴㅔㅇㅛ':'ㄴㅔ',\n",
    "    'ㄴㅡㄴㄷㅔㅇㅛ':'ㄴㅡㄴㄷㅔ',\n",
    "    'ㄱㅓㄷㅡㄴㅇㅛ':'ㄱㅓㄷㅡㄴ',\n",
    "    'ㄱㅜㄴㅇㅛ': 'ㄱㅜㄴㅏ',\n",
    "    'ㅇㅡㄴㄷㅔㅇㅛ':'ㅇㅡㄴㄷㅔ',\n",
    "    'ㅈㅏㄱㅜㅇㅛ':'ㅈㅏㄱㅜ',\n",
    "    'ㄴㅣㄲㅏㅇㅛ': 'ㄴㅣㄲㅏ',\n",
    "    'ㅈㅣㅇㅛ':'ㅈㅣ',\n",
    "    \n",
    "    #의문문\n",
    "    'ㄴㅏㅇㅛ':'special6',\n",
    "    'ㄹㄲㅏㅇㅛ':'ㄹㄲㅏ',\n",
    "    'ㅇㅡㄹㄲㅏㅇㅛ':'ㅇㅡㄹㄲㅏ',\n",
    "    'ㄴㄱㅏㅇㅛ':'ㄴㄱㅏ',\n",
    "    'ㄹㄹㅐㅇㅛ':'ㄹㄹㅐ',\n",
    "    'ㄹㅐㅇㅛ':'ㄹㅐ',\n",
    "    'ㄱㅗㅇㅛ':'ㄱㅗ',\n",
    "    'ㅇㅡㄴㄱㅏㅇㅛ':'ㅇㅡㄴㄱㅏ',\n",
    "    'ㅇㅣㄴㄱㅏㅇㅛ':'ㅇㅣㄴㄱㅏ',\n",
    "}\n",
    "need_origin_EF = {\n",
    "    'ㅂㅅㅣㄷㅏ':'ㅈㅏ',\n",
    "}\n",
    "#'ㄹ'규칙 활용 -> ㄹ 규칙 활용이 일어나는 동사들을 최대한 모아둔 Dictionary\n",
    "EF_R_rule= {\n",
    "    'ㄱㅜ':'ㄹ',\n",
    "    'ㄴㅗ':'ㄹ',\n",
    "    'ㄴㅏ':'ㄹ',\n",
    "    'ㄷㅗ':'ㄹ',\n",
    "    'ㄷㅡ':'ㄹ',\n",
    "    'ㄷㅏ':'ㄹ',\n",
    "    'ㄷㅜ':'ㄹ',\n",
    "    'ㅂㅜ':'ㄹ',\n",
    "    'ㄲㅗ':'ㄹ',\n",
    "    'ㅁㅣ':'ㄹ',\n",
    "    'ㅁㅜ':'ㄹ',\n",
    "    #'ㅂㅗㅍㅜ':'ㄹ', #error predicate 수정\n",
    "    'ㅂㅜ':'ㄹ',\n",
    "    'ㅅㅡ':'ㄹ',\n",
    "    'ㄸㅓ':'ㄹ',   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mecab & Khaiii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mec = Mecab()\n",
    "khai = KhaiiiApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_h(input, lis_end_h, lis_end_l):\n",
    "    for i in lis_end_h:\n",
    "        if len(input)>=len(i):\n",
    "            if input[-len(i):]==i:\n",
    "                return 1\n",
    "            \n",
    "    for i in lis_end_l:\n",
    "        if len(input)>=len(i):\n",
    "            if input[-len(i):]==i:\n",
    "                return 0\n",
    "            \n",
    "    return -1\n",
    "\n",
    "def unite(input, dict):\n",
    "    for i in dict:\n",
    "        input = re.sub(i[0],i[1],input)\n",
    "    return input\n",
    "    \n",
    "## 자모 단위로 문장을 나누고 합칠 때 쓰는 class ##\n",
    "class Jamodealer:\n",
    "    jamo = []\n",
    "    pp = ''\n",
    "    #각 단어들을 받아와서 자모단위로 나눈다.\n",
    "    def __init__(self,lis_word):\n",
    "    \n",
    "        self.jamo = []\n",
    "        for i in lis_word:\n",
    "            self.jamo.append(split_syllables(i))\n",
    "    \n",
    "    ##사전에서 변환된 자모단위로 분리된 문장을 합칠 때 쓰는 함수이다.     \n",
    "    def make_one(self):\n",
    "        #list 형태로 저장된 자모들의 집합을 하나의 string pp에 저장한다. \n",
    "        self.pp = ''\n",
    "        for i in self.jamo:\n",
    "             self.pp= self.pp+i\n",
    "        ##종성과 종성을 합쳐야 하는 경우가 있다면 합친다.        \n",
    "        self.pp = unite(self.pp, con_dict)\n",
    "        \n",
    "        #자모 단위의 string에서 자모 단위로 사전을 만들고 거기에 index를 부여한다.        \n",
    "        chars = list(set(self.pp))\n",
    "        char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "        ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "        \n",
    "        #자모 단위로 분리되었던 문장을 다시 하나로 합친다.\n",
    "        jamo_numbers = [char_to_ix[x] for x in self.pp]\n",
    "        restored_jamo = ''.join([ix_to_char[x] for x in jamo_numbers])\n",
    "        #합쳐진 문장을 return 한다.\n",
    "        restored_text = join_jamos(restored_jamo)\n",
    "        return restored_text\n",
    "\n",
    "def to2lists(input):\n",
    "    lis_word = []\n",
    "    lis_tag = []\n",
    "    #data = han.pos(input,ntags=22,flatten=True, join=False)\n",
    "    data = mec.pos(input)\n",
    "    for i in data:\n",
    "        lis_word.append(i[0])\n",
    "        lis_tag.append(i[1])\n",
    "    return lis_word, lis_tag\n",
    "\n",
    "def to2lists_khaiii(input):\n",
    "    lis_word = []\n",
    "    lis_tag = []\n",
    "    analyzed = khai.analyze(input)  \n",
    "    for data in analyzed:\n",
    "        for morph in data.morphs:\n",
    "            lis_word.append(morph.lex)\n",
    "            lis_tag.append(morph.tag)\n",
    "    return lis_word, lis_tag\n",
    "\n",
    "\n",
    "def rememberSpace(lis, input):\n",
    "    \n",
    "    rlis = []\n",
    "    \n",
    "    for i in range(len(lis)):\n",
    "        if lis[i]==input:\n",
    "            rlis.append(i)\n",
    "            \n",
    "    for i in range(len(rlis)):\n",
    "        rlis[i] = rlis[i]-i      \n",
    "    return rlis\n",
    "\n",
    "def convertSpace(lis_space,lis_lis):\n",
    "    \n",
    "    rlis = []\n",
    "    k=0\n",
    "    for i in range(len(lis_lis)):\n",
    "        \n",
    "        if k in lis_space:\n",
    "            rlis.append(i)\n",
    "            \n",
    "        k = k+len(lis_lis[i])\n",
    "        \n",
    "    #print(rlis)  \n",
    "    return rlis\n",
    "\n",
    "def union(lis, lis_lis):\n",
    "    \n",
    "    k = 0\n",
    "    for i in lis:\n",
    "        lis_lis.insert(i+k,' ')\n",
    "        k = k+1\n",
    "\n",
    "def union_t_03(lis_tag):\n",
    "    \n",
    "    for i in range(1, len(lis_tag)):\n",
    "        if lis_tag[i-1] ==' ' or lis_tag[i]==' ':\n",
    "            lis_tag[i] = lis_tag[i]\n",
    "        else:\n",
    "            lis_tag[i] = '/'+lis_tag[i]\n",
    "            \n",
    "def union_w_03(lis_w, lis_tag):\n",
    "    \n",
    "    for i in range(1, len(lis_w)):\n",
    "        if lis_tag[i]==' SF':\n",
    "            lis_w[i] = ' '+lis_w[i+1]\n",
    "def proc_khaiii_with_Tag(input):\n",
    "    \n",
    "    r_sen = input\n",
    "    \n",
    "    res1 = ''\n",
    "    res2 =''\n",
    "    slis = []\n",
    "    for i in range(len(input)):\n",
    "        if r_sen[i]==' ':\n",
    "            slis.append(1)\n",
    "        elif r_sen[i]=='  ':\n",
    "            slis.append(2)\n",
    "            \n",
    "    wlis = r_sen.split(' ')\n",
    "    \n",
    "    uu = khai.analyze(wlis[0])\n",
    "    \n",
    "    elem = ''\n",
    "    tag =''\n",
    "    \n",
    "    for data in uu[0].morphs:\n",
    "        elem = elem + data.lex+'/'\n",
    "        tag = tag+data.tag+'/'\n",
    "    \n",
    "    res1 = res1+elem\n",
    "    res2 = res2+tag\n",
    "    \n",
    "    for i in range(len(slis)):\n",
    "        elem = ''\n",
    "        tag = ''\n",
    "        elem = elem+slis[i]*' '\n",
    "        if i != len(wlis)-1:\n",
    "            uu = khai.analyze(wlis[i+1])\n",
    "            for data in uu[0].morphs:\n",
    "                elem = elem+data.lex+'/'\n",
    "                tag = tag+data.tag+'/'\n",
    "        res1 = res1+elem\n",
    "        res2 = res2+tag\n",
    "    return res1,res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_khaiii(input):\n",
    "    lis_w, lis_t = to2lists_khaiii(input)\n",
    "\n",
    "    space_list = rememberSpace(input,' ')\n",
    "    space_location = convertSpace(space_list, lis_w)\n",
    "    union(space_location, lis_w)\n",
    "    union(space_location, lis_t)\n",
    "    union_t_03(lis_t)\n",
    "    union_w_03(lis_w, lis_t)\n",
    "    \n",
    "    str_w = ''\n",
    "    str_t = ''\n",
    "    for i in range(len(lis_w)):\n",
    "        str_w = str_w + lis_w[i]\n",
    "        str_t = str_t + lis_t[i]\n",
    "    \n",
    "    data_w = str_w.split(' ')\n",
    "    data_t = str_t.split(' ')\n",
    "    \n",
    "    lis_word, lis_tag = to2lists_khaiii(input)\n",
    "    \n",
    "    lis_ind = []\n",
    "    t_ind = 0\n",
    "    jam1 = Jamodealer(lis_word)\n",
    "    jam2 = Jamodealer(data_w)\n",
    "    for i in range(len(data_w)):\n",
    "        element = []\n",
    "        leng = len(data_t[i].split('/'))\n",
    "        res = jam2.jamo[i]\n",
    "        ind = 0\n",
    "        lenlen = 0\n",
    "        #element.append(0)\n",
    "        for j in range(leng):\n",
    "            element.append(ind)\n",
    "            ind = ind + len(jam1.jamo[t_ind])\n",
    "            res = res[len(jam1.jamo[t_ind]):]\n",
    "            \n",
    "            lenlen = len(jam1.jamo[t_ind])+lenlen\n",
    "            t_ind = t_ind+1\n",
    "\n",
    "        element.append(len(jam2.jamo[i]))\n",
    "        lis_ind.append(element)\n",
    "        \n",
    "    return data_w, data_t, lis_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_beta_khaiii(input, lis_ef, tag_last, lis_w_last, lis_w_last_not):\n",
    "    data_w, data_t, lis_ind = prepro_khaiii(input)\n",
    "    \n",
    "    last_words = []\n",
    "\n",
    "    data_w_jamo = []\n",
    "\n",
    "    data_t_after = []\n",
    "    \n",
    "    lis_target_ind = []\n",
    "    \n",
    "    for i in data_w:\n",
    "        jam_ele = Jamodealer(i)\n",
    "        ele = ''\n",
    "        for j in jam_ele.jamo:\n",
    "            ele = ele+j\n",
    "        data_w_jamo.append(ele)\n",
    "    \n",
    "    for i in range(len(data_t)):\n",
    "        #if i<len(data_t)-1:\n",
    "        if i<len(data_t):\n",
    "            lis_res = []\n",
    "            for ind in range(len(lis_ind[i])-1):\n",
    "                lis_res.append(data_w_jamo[i][lis_ind[i][ind]:lis_ind[i][ind+1]])\n",
    "\n",
    "        \n",
    "        if 'EF/SF' in data_t[i] or 'EF/SV' in data_t[i] or 'UNKNOWN/SF' in data_t[i] or 'UNKNOWN/SV' in data_t[i]:# and 'EC/SF' not in data_t[i]:\n",
    "            if 'EF/SF' in data_t[i] or 'UNKNOWN' in data_t[i]:\n",
    "                elements = data_t[i].split('/')\n",
    "                flag = 0\n",
    "\n",
    "                for j in range(len(elements)):\n",
    "                \n",
    "                    flag_end = detect_h(lis_res[j], lis_w_last,  lis_w_last_not)\n",
    "\n",
    "                    if elements[j] in lis_ef and flag_end==1: #and j == len(elements)-1:\n",
    "\n",
    "                        elements[j] = 'NULL'\n",
    "                    \n",
    "                        last_words.append(data_w_jamo[i][lis_ind[i][j]:lis_ind[i][j+1]])\n",
    "                        lis_res[j]=''\n",
    "                    \n",
    "                        lis_target_ind.append(i)\n",
    "                    \n",
    "                        \n",
    "                    elif 'EF' in elements[j] and flag_end==1:# + EF를 처리하는 부분이므로 + EF 만을 마지막에서 처리한다.\n",
    "                        for jam in lis_w_last:\n",
    "                            if len(lis_res[j])>=len(jam):\n",
    "\n",
    "                                res_out_punc = lis_res[:lis_ind[i][-2]][j]\n",
    "\n",
    "                                if res_out_punc[-len(jam):]==jam:\n",
    "\n",
    "                                    #print(jam)\n",
    "                                    lis_target_ind.append(i)\n",
    "                                \n",
    "                                    last_words.append(jam)\n",
    "                                    lis_res[j] = lis_res[j].replace(jam, '', 1)\n",
    "\n",
    "                                    for k in tag_last:\n",
    "\n",
    "                                        if k in elements[j]:\n",
    "                                        \n",
    "                                            if '+' in elements[j]:\n",
    "                                            \n",
    "                                                ind = elements[j].index('+'+k)\n",
    "                                                elements[j] = elements[j][:ind]\n",
    "                                    break\n",
    "                                \n",
    "                                elif lis_w_last.index(jam)==len(lis_w_last)-1:#new\n",
    "                                    for k in tag_last:\n",
    "\n",
    "                                        if k in elements[j]:\n",
    "                                        \n",
    "                                            if '+' in elements[j]:\n",
    "                                            \n",
    "                                                ind = elements[j].index('+'+k)\n",
    "                                                elements[j] = elements[j][:ind]\n",
    "                                            \n",
    "                                    lis_target_ind.append(i)\n",
    "                                    last_words.append('')\n",
    "                                    break\n",
    "\n",
    "                \n",
    "                elements_post = '/'.join(elements)\n",
    "                data_t_after.append(elements_post)\n",
    "                \n",
    "                data_w_jamo[i] = ''.join(lis_res)\n",
    "\n",
    "                #elements_post = '/'.join(elements)\n",
    "                #data_t_after.append(elements_post)\n",
    "                    \n",
    "            #######################################\n",
    "            \n",
    "            #data_t_after.append(data_t[i])\n",
    "            \n",
    "        elif 'EC/SF' in data_t[i] or 'JX/SF' in data_t[i]:\n",
    "            #print('ee')\n",
    "            elements = data_t[i].split('/')\n",
    "            \n",
    "            flag = 0\n",
    "            for j in range(len(elements)):\n",
    "                \n",
    "                flag_end = -1\n",
    "                if 'EC' in elements[j] or 'JX' in elements[j]:\n",
    "                    flag_end = detect_h(lis_res[j], lis_w_last, lis_w_last_not)\n",
    "\n",
    "                #print(flag_end)\n",
    "                if flag_end==1 and i not in lis_target_ind:\n",
    "                    for jam in lis_w_last:\n",
    "\n",
    "                        if len(jam)<=len(lis_res[j]):\n",
    "                            #print(lis_res[j])\n",
    "                            if lis_res[j][-len(jam):]==jam:\n",
    "\n",
    "                                last_words.append(jam)\n",
    "                                lis_res[j] = lis_res[j].replace(jam, '', 1)\n",
    "                                #print(lis_res[j])\n",
    "                    lis_target_ind.append(i)\n",
    "                    #last_words.append('')\n",
    "            data_t_after.append(data_t[i])\n",
    "            data_w_jamo[i] = ''.join(lis_res)\n",
    "        ####### #######\n",
    "            \n",
    "        else:\n",
    "            data_t_after.append(data_t[i])\n",
    "            \n",
    "    \n",
    "    \n",
    "        lis_normal = []\n",
    "    \n",
    "        for i in data_w_jamo:\n",
    "            jam_n = Jamodealer(i)\n",
    "            lis_normal.append(jam_n.make_one())\n",
    "    \n",
    "        \n",
    "        \n",
    "    for i in range(len(lis_target_ind)):\n",
    "        if 'ㅅㅔㅇㅛ' == last_words[i] or 'ㄹㄹㅐㅇㅛ' == last_words[i]:\n",
    "            \n",
    "            for wk in lis_wk:\n",
    "                #print(data_w_jamo[lis_target_ind[i]][-len(wk[0])-1:-1])\n",
    "                if data_w_jamo[lis_target_ind[i]][-len(wk[0])-1:-1] ==wk[0]:\n",
    "                    #print('rrrr')\n",
    "                    ele = data_w_jamo[lis_target_ind[i]][:-len(wk[0])-1]\n",
    "                    ele = ele + wk[1]\n",
    "                    ele = ele + data_w_jamo[lis_target_ind[i]][-1]\n",
    "                    data_w_jamo[lis_target_ind[i]] = ele\n",
    "                    \n",
    "                    break\n",
    "        \n",
    "    return data_w, data_t, lis_ind, data_w_jamo, data_t_after, last_words, lis_target_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_ch03(input, lis_ef, tag_last, lis_w_last, lis_w_last_not, lis_ic):\n",
    "    \n",
    "    lis_res_word = []\n",
    "    \n",
    "    lis_input = input.split()\n",
    "    lis_word = []\n",
    "    lis_tag = []\n",
    "    lis_last_word = []\n",
    "    lis_ind = []\n",
    "    \n",
    "    lis_result = []\n",
    "    \n",
    "    for i in range(len(lis_input)):\n",
    "        ele_w = []\n",
    "        ele_t = []\n",
    "        \n",
    "        an = mec.pos(lis_input[i])\n",
    "        for j in range(len(an)):\n",
    "            ele_w.append(an[j][0])\n",
    "            ele_t.append(an[j][1])\n",
    "        elem_w = ' '.join(ele_w)\n",
    "        elem_t = '/'.join(ele_t)\n",
    "        \n",
    "        jam_pre = Jamodealer(elem_w)\n",
    "        lis_word.append(''.join(jam_pre.jamo))\n",
    "        lis_tag.append(elem_t)\n",
    "    \n",
    "    for i in range(len(lis_tag)):\n",
    "        \n",
    "        if 'EF/SF' in lis_tag[i] or 'EF/SV' in lis_tag[i] or 'EF/SC' in lis_tag[i] or 'JX/SF' in lis_tag[i] or 'JX/SC' in lis_tag[i] or 'IC' in lis_tag[i] or 'IC' in lis_tag[i] or (('NP' in lis_tag[i] or 'NNG' in lis_tag[i]) and ('ㅈㅓ' in lis_word[i] or 'ㅈㅔ' in lis_word[i] or 'ㄷㅏㅇㅅㅣㄴ' in lis_word[i])):\n",
    "            elemen_t = lis_tag[i].split('/')\n",
    "            elemen_w = lis_word[i].split(' ')\n",
    "            flag = 0\n",
    "            \n",
    "            \n",
    "            for j in range(len(elemen_t)):\n",
    "\n",
    "                flag_end = detect_h(elemen_w[j], lis_w_last_not, lis_w_last)\n",
    "                \n",
    "                \n",
    "                if (flag_end == 1 and 'EF' in elemen_t[j]) or (flag_end == 1 and 'JX' in elemen_t[j]):\n",
    "                    #@\n",
    "                    #print(flag_end)\n",
    "                    for jam in lis_w_last_not:\n",
    "                        \n",
    "                        if len(elemen_w[j])>=len(jam):\n",
    "                            \n",
    "                            if elemen_w[j][-len(jam):]==jam:\n",
    "                                \n",
    "                                \n",
    "                                lis_ind.append(i)\n",
    "                                lis_last_word.append(jam)\n",
    "                                elemen_w[j] = elemen_w[j][:-len(jam)]\n",
    "                                elemen_w[j] = elemen_w[j] + '__+__'\n",
    "                                \n",
    "                                break\n",
    "                                \n",
    "                ############# 인칭 대명사 변경 #############                \n",
    "                ## 인칭 대명사를 사전에 미리 변경한다                \n",
    "                elif (('NP' in lis_tag[i] or 'NNG' in lis_tag[i]) and ('ㅈㅓ' in lis_word[i] or 'ㅈㅔ' in lis_word[i] or 'ㄷㅏㅇㅅㅣㄴ' in lis_word[i])) or ('IC' in lis_tag[i] and('ㄴㅔ' in lis_word[i] or 'ㅇㅏㄴㅣㅇㅛ' in lis_word[i])):\n",
    "                    \n",
    "                    jam1 = Jamodealer(elemen_w)\n",
    "                    s = jam1.make_one()\n",
    "\n",
    "                    key_u = 0\n",
    "\n",
    "                    ss = proc_khaiii_with_Tag(s)\n",
    "                    tagt = ss[1].split('/')[:-1]\n",
    "                    wordw = ss[0].split('/')[:-1]\n",
    "\n",
    "                    for u in range(len(tagt)):\n",
    "                        if tagt[u]=='NP':\n",
    "                            if wordw[u]=='저' or wordw[u]=='제':\n",
    "                                key_u = 1\n",
    "                            elif wordw[u]=='당신':\n",
    "                                key_u = 2\n",
    "                        elif tagt[u]=='IC':\n",
    "                            if wordw[u]=='네':\n",
    "                                key_u = 3\n",
    "                        elif tagt[u]=='VCN' or tagt[u]=='IC':\n",
    "                            if wordw[u][:2]=='아니':\n",
    "                                key_u = 4\n",
    "            \n",
    "                    flag = 0\n",
    "                    for ind in range(len(elemen_t)):\n",
    "                        #print(elemen_w[ind])\n",
    "                        if flag==0 and key_u>0:\n",
    "\n",
    "                            if key_u == 1:\n",
    "\n",
    "                                if elemen_t[ind][:2]=='NP' or elemen_t[ind][:3]=='NNG':\n",
    "                                    \n",
    "                                    if elemen_w[ind][:2] == 'ㅈㅓ':\n",
    "                                        \n",
    "                                        elemen_w[ind] = 'ㄴㅏ' + elemen_w[ind][2:]\n",
    "                                        flag = 1\n",
    "\n",
    "                                    elif elemen_w[ind][:2] == 'ㅈㅔ':\n",
    "\n",
    "                                        elemen_w[ind] = 'ㄴㅐ' + elemen_w[ind][2:]\n",
    "                                        flag = 1\n",
    "                            \n",
    "                            elif key_u==2:\n",
    "                                if len(elemen_w[ind])>=6:\n",
    "                                    if elemen_w[ind][:6] == 'ㄷㅏㅇㅅㅣㄴ':\n",
    "\n",
    "                                        elemen_w[ind] = 'ㄴㅓ' + elemen_w[ind][6:]\n",
    "                                        flag = 1\n",
    "                                        if len(elemen_t)-1 >ind:\n",
    "                                            if elemen_t[ind+1] == 'JX' or elemen_t[ind+1] == 'JKS' or elemen_t[ind+1] == 'JKO' or elemen_t[ind+1] == 'JKB':\n",
    "                                                if elemen_w[ind+1] == 'ㅇㅡㄹ':\n",
    "                                                    elemen_w[ind+1] = 'ㄹㅡㄹ'\n",
    "                                                elif elemen_w[ind+1] == 'ㅇㅡㄴ':\n",
    "                                                    elemen_w[ind+1] = 'ㄴㅡㄴ'\n",
    "                                                elif elemen_w[ind+1] == 'ㅇㅣ':\n",
    "                                                    elemen_w[ind+1] = 'ㄱㅏ'\n",
    "                                                elif elemen_w[ind+1] == 'ㄱㅘ':\n",
    "                                                    elemen_w[ind+1] = 'ㅇㅘ'\n",
    "                                                    \n",
    "                            elif key_u==3:\n",
    "                                if len(elemen_t)>1:\n",
    "                                    \n",
    "                                    if elemen_t[1]=='SC' or elemen_t[1]=='SF':\n",
    "                                        if elemen_w[ind][:2] == 'ㄴㅔ':\n",
    "                                        \n",
    "                                            elemen_w[ind] = 'ㅇㅡㅇ' + elemen_w[ind][2:]\n",
    "                                            flag = 1\n",
    "                            else:\n",
    "                                if elemen_t[ind][:2] == 'IC':\n",
    "\n",
    "                                    if elemen_w[ind][4:6] == 'ㅇㅛ':\n",
    "\n",
    "                                        elemen_w[ind] = elemen_w[ind][:4] + elemen_w[ind][6:]\n",
    "\n",
    "                                        flag = 1\n",
    "\n",
    "                    \n",
    "                                \n",
    "            res_w = ''.join(elemen_w)\n",
    "            lis_result.append(res_w)\n",
    "                                \n",
    "        else:\n",
    "            \n",
    "            rere = lis_word[i].split(' ')\n",
    "            \n",
    "            resres = ''.join(rere)\n",
    "            \n",
    "            lis_result.append(resres)\n",
    "            \n",
    "            \n",
    "    return lis_result, lis_tag, lis_ind, lis_last_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def treatSF(stc, ex):\n",
    "    ind_point = -1\n",
    "    point = ''\n",
    "    if '__+__' in stc:\n",
    "        ind_point = stc.index('__+__')\n",
    "        stc = stc.replace('__+__', '', 1)\n",
    "    \n",
    "    r_word = ''\n",
    "    r_pun = ''\n",
    "    \n",
    "    if ind_point!=-1:\n",
    "        r_word = stc[:ind_point]\n",
    "        r_pun = stc[ind_point:]\n",
    "    else:\n",
    "        r_word = stc\n",
    "    return r_word+ex+r_pun\n",
    "\n",
    "def delete_EP_si(stn, taglist):\n",
    "    si = stn[-3:-1]\n",
    "    eusi = stn[-5:-1]\n",
    "    check_si = taglist[-11:-2]\n",
    "    #@\n",
    "    \n",
    "    result =''\n",
    "    flag = 0\n",
    "    if taglist.find('SF') !=-1:\n",
    "        if (eusi =='ㅇㅡㅅㅣ') and (check_si.find('EP+EP')!=-1 or check_si.find('EP/NULL')!=-1):\n",
    "            result = stn[:-5]+stn[-1]\n",
    "            flag = 1\n",
    "        elif (si =='ㅅㅣ') and (check_si.find('/EP/')!=-1):\n",
    "            #@\n",
    "            #print('alalalalal')\n",
    "            result = stn[:-3]+stn[-1]\n",
    "            #print('s')\n",
    "\n",
    "    return result, flag\n",
    "\n",
    "def check_VV_VA(sentence, tag):\n",
    "    #t= tag[-9:-3]\n",
    "    #print(t, tag)\n",
    "    tt = tag.split('/SF')\n",
    "    t= tt[0][-6:]\n",
    "    if 'VV' in t or 'VX' in t or 'XSV' in t or ('VV/EP' in tag and 'ㅅㅣ' in sentence[-5:-1]) :\n",
    "        return 1\n",
    "    elif 'VA' in t:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "## 종결어미 자리를 뜻하는 문자를 없애는 함수. 어체 간 변환을 할 때 사용됨\n",
    "def detach_endmark(sentence):\n",
    "    endmark = sentence[-6:-1]\n",
    "    #print(sentence, endmark)\n",
    "    if endmark == '__+__':\n",
    "        sentence = sentence[:-6]+sentence[-1]\n",
    "    else:\n",
    "        endmark = -1\n",
    "    return sentence, endmark\n",
    "def attach_endmark(sentence):\n",
    "    endmark = '__+__'\n",
    "    sentence = sentence[:-1]+endmark+sentence[-1]\n",
    "    return sentence\n",
    "\n",
    "\n",
    "\n",
    "##if Verb & adjective\n",
    "## '-시' 등 선어말 처리\n",
    "## 습니다는 동사면 '는다', 그외에는 '다'로 간다\n",
    "def rememberSpace_k(lis, input):\n",
    "    \n",
    "    rlis = []\n",
    "    \n",
    "    for i in range(len(lis)):\n",
    "        if lis[i]==input:\n",
    "            rlis.append(i)\n",
    "            \n",
    "    for i in range(len(rlis)):\n",
    "        rlis[i] = rlis[i]-i      \n",
    "    return rlis\n",
    "\n",
    "def convertSpace_k(lis_space,lis_lis):\n",
    "    \n",
    "    rlis = []\n",
    "    k=0\n",
    "    for i in range(len(lis_lis)):\n",
    "        \n",
    "        if k in lis_space:\n",
    "            rlis.append(i)\n",
    "            \n",
    "        k = k+len(lis_lis[i])\n",
    "        \n",
    "    #print(rlis)  \n",
    "    return rlis\n",
    "\n",
    "def proc_khaiii(input):\n",
    "    \n",
    "    r_sen = input\n",
    "    \n",
    "    res = ''\n",
    "    slis = []\n",
    "    for i in range(len(input)):\n",
    "        if r_sen[i]==' ':\n",
    "            slis.append(1)\n",
    "        elif r_sen[i]=='  ':\n",
    "            slis.append(2)\n",
    "            \n",
    "    wlis = r_sen.split(' ')\n",
    "    \n",
    "    uu = khai.analyze(wlis[0])\n",
    "    \n",
    "    elem = ''\n",
    "    \n",
    "    for data in uu[0].morphs:\n",
    "        elem = elem + data.lex\n",
    "    \n",
    "    res = res+elem\n",
    "    \n",
    "    for i in range(len(slis)):\n",
    "        elem = ''\n",
    "        elem = elem+slis[i]*' '\n",
    "        if i != len(wlis)-1:\n",
    "            uu = khai.analyze(wlis[i+1])\n",
    "            for data in uu[0].morphs:\n",
    "                elem = elem+data.lex\n",
    "        res = res+elem\n",
    "    return res\n",
    "\n",
    "def proc_khaiii_with_Tag(input):\n",
    "    \n",
    "    r_sen = input\n",
    "    \n",
    "    res1 = ''\n",
    "    res2 =''\n",
    "    slis = []\n",
    "    for i in range(len(input)):\n",
    "        if r_sen[i]==' ':\n",
    "            slis.append(1)\n",
    "        elif r_sen[i]=='  ':\n",
    "            slis.append(2)\n",
    "            \n",
    "    wlis = r_sen.split(' ')\n",
    "    \n",
    "    uu = khai.analyze(wlis[0])\n",
    "    \n",
    "    elem = ''\n",
    "    tag =''\n",
    "    \n",
    "    for data in uu[0].morphs:\n",
    "        elem = elem + data.lex+'/'\n",
    "        tag = tag+data.tag+'/'\n",
    "    \n",
    "    res1 = res1+elem\n",
    "    res2 = res2+tag\n",
    "    \n",
    "    for i in range(len(slis)):\n",
    "        elem = ''\n",
    "        tag = ''\n",
    "        elem = elem+slis[i]*' '\n",
    "        if i != len(wlis)-1:\n",
    "            uu = khai.analyze(wlis[i+1])\n",
    "            for data in uu[0].morphs:\n",
    "                elem = elem+data.lex+'/'\n",
    "                tag = tag+data.tag+'/'\n",
    "        res1 = res1+elem\n",
    "        res2 = res2+tag\n",
    "    return res1,res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Changer_low(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.re_value=''\n",
    "    \n",
    "    def __convert_EF(self, sentence, taglist, ending):\n",
    "        re_value =''\n",
    "        temp =''\n",
    "        ni = 0\n",
    "        flag = 0\n",
    "        \n",
    "        #존칭 동사 또는 보조 동사를 파악하고 이를 변환 전에 미리 바꿔주어야 한다. \n",
    "        for key in EF:\n",
    "            if ending == key:\n",
    "                flag = 1\n",
    "                re_value = EF[key]\n",
    "                #나요, 으세요(으세요, 세요 모두 함수에서 커버 가능), 습니까 case\n",
    "                if re_value == 'special0':\n",
    "                    #시처리하기\n",
    "                    after_si, ni = delete_EP_si(sentence, taglist)\n",
    "                    if after_si != '':\n",
    "                        sentence = after_si\n",
    "                        after_si =''\n",
    "                    temp = self.__convertSpecialCase_AhOh(sentence)\n",
    "                    re_value = treatSF(sentence, temp)\n",
    "                    #ㅗ,ㅜ 이면 ㅘ, ㅝ로 결합할 것\n",
    "                #-세요,십니다, 십니까\n",
    "                elif re_value == 'special1':\n",
    "                    #시처리하기\n",
    "                    after_si, ni = delete_EP_si(sentence, taglist)\n",
    "                    if after_si != '':\n",
    "                        sentence = after_si\n",
    "                        after_si =''\n",
    "                    re_value = self.__convertSpecialCase_SaeYo(sentence, ending, taglist)\n",
    "                elif re_value == 'special2':\n",
    "                    re_value = self.__convertSpecialCase_Da(sentence, ending, taglist)\n",
    "                elif re_value == 'special3':\n",
    "                    #시처리 안함 '시' 보존\n",
    "                    re_value = self.__convertSpecialCase_Nida(sentence, ending, taglist)\n",
    "                elif re_value == 'special4':\n",
    "                    re_value = self.__convertSpecialCase_SaeYo(sentence, ending, taglist)\n",
    "                    #print(re_value)\n",
    "                    if ending == 'ㅇㅡㅂㅅㅣㄷㅏ':\n",
    "                        temp = self.__convertSpecialCase_AhOh(sentence)\n",
    "                        re_value = treatSF(sentence, temp)\n",
    "                elif re_value == 'special5':\n",
    "                    re_value = self.__convertSpecialCase_Yo(sentence, ending, taglist)\n",
    "                elif re_value == 'special6':\n",
    "                    re_value = self.__convertSpecialCase_NaYo(sentence, ending, taglist)\n",
    "                else:\n",
    "                    #위험하기 때문에 데이터 확인 후 수정(보류)\n",
    "#                     if taglist.find('EP') !=-1 and sentence[-4:].find('ㅅㅣ.'):\n",
    "#                         sentence = delete_EP_si(sentence, taglist)\n",
    "#                         re_value = convertSpecialCase_SaeYo(sentence, ending)\n",
    "#                     else:\n",
    "#                         re_value = treatSF(sentence,re_value)\n",
    "                    re_value = treatSF(sentence,re_value)\n",
    "        if flag ==0:\n",
    "            return treatSF(sentence, ending)\n",
    "        return re_value\n",
    "    \n",
    "    def __convert_IC(self, sentence, taglist, ending):\n",
    "        IC = {'ㅇㅖ':'ㅇㅡㅇ', 'ㄴㅔ':'ㅇㅡㅇ', 'ㅇㅏㄴㅣㅇㅗ':'ㅇㅏㄴㅣ', 'ㅇㅏㄴㅣㅇㅛ':'ㅇㅏㄴㅣ'}\n",
    "        re_value = ''\n",
    "        flag = 0\n",
    "        for key in IC:\n",
    "            if ending == key:\n",
    "                re_value = IC[key]\n",
    "                flag = 1\n",
    "        if flag != 1:\n",
    "            re_value = ending\n",
    "        re_value = treatSF(re_value, sentence)\n",
    "        return re_value\n",
    "    \n",
    "    def __make_end_low(self, sentence, taglist, ending):\n",
    "        re_value =''\n",
    "        sentence = self.__treatFormal_vv(sentence)\n",
    "        if taglist.find('IC') !=-1 and len(taglist)<6:\n",
    "             re_value = self.__convert_IC(sentence, taglist, ending)\n",
    "        else:\n",
    "            re_value = self.__convert_EF(sentence, taglist, ending)\n",
    "        return re_value\n",
    "    \n",
    "    def to_low(self, input):\n",
    "\n",
    "        if '  ' in input:\n",
    "            return input\n",
    "        result = input\n",
    "\n",
    "        converted_w, converted_t, target_ind ,last_ef = prepro_ch03(result, lis_beta_ef, lis_tag_last, lis_end_2low, lis_end, lis_ic)\n",
    "\n",
    "\n",
    "        if len(target_ind)!=0:\n",
    "\n",
    "            for i in range(len(target_ind)):\n",
    "                \n",
    "                new_end = self.__make_end_low(converted_w[target_ind[i]], converted_t[target_ind[i]], last_ef[i])\n",
    "                \n",
    "                converted_w[target_ind[i]] = new_end\n",
    "            \n",
    "            res = ' '.join(converted_w)\n",
    "            jam = Jamodealer(res)\n",
    "            return jam.make_one()\n",
    "\n",
    "        return input\n",
    "    \n",
    "    \n",
    "    def __convertSpecialCase_AhOh(self, stc):\n",
    "    ## 수정할 필요 있음!\n",
    "    #danger\n",
    "        sentence, endmark = detach_endmark(stc)\n",
    "        if sentence[-3:-1] == 'ㅍㅡ' or sentence[-3:-1] == 'ㅃㅡ':\n",
    "            if sentence[-5:-3].find('ㅏ') !=-1 or sentence[-5:-3].find('ㅗ')!=-1:\n",
    "                return 'ㅏ'\n",
    "            else:\n",
    "                return 'ㅓ'\n",
    "        elif sentence[-3:].find('ㅏ') !=-1 or sentence[-3:].find('ㅗ') !=-1:\n",
    "            return 'ㅇㅏ'\n",
    "        else:\n",
    "            return 'ㅇㅓ'\n",
    "        \n",
    "    # 일부 주체 높임 어휘 처리\n",
    "    def __treatFormal_vv(self, sentence):\n",
    "        formal_vv ={\n",
    "            'ㄱㅖ':'ㅇㅣㅆㅇㅡ',\n",
    "            'ㅈㅜㅁㅜ':'ㅈㅏ',\n",
    "            'ㅈㅏㅂㅅㅜ':'ㅁㅓㄱㅇㅡ'\n",
    "        }\n",
    "        for key in formal_vv:\n",
    "            if sentence.find(key) !=-1:\n",
    "                sentence = sentence.replace(key,formal_vv[key])\n",
    "        return sentence\n",
    "    def __check_NoRule(self, stem,predicate):\n",
    "        #this is temp function. Need to modify this function\n",
    "        if predicate.find('ㅓ') !=-1:\n",
    "            if (stem+predicate) =='ㄱㅡㄹㅓ':\n",
    "                return 'ㅐ'\n",
    "        else:\n",
    "            return ''\n",
    "    #'ㅇㅗㄹㅡ'\n",
    "    def __convertSpecialCase_SaeYo(self, stc, ending, tag):\n",
    "        result = ''\n",
    "        end_EF=''\n",
    "        final=''\n",
    "        sentence, endmark = detach_endmark(stc)\n",
    "\n",
    "        pun = sentence[-1:]\n",
    "        predicate = sentence[-3:-1]\n",
    "        stem = sentence[:-3]\n",
    "        isVcp = tag[-12:]\n",
    "\n",
    "        #만약 VCP가 있다면 '야'를 붙이고 return 한다.\n",
    "        if isVcp.find('VCP/EP+EF') != -1 or isVcp.find('VCP+EP+EF') !=-1:\n",
    "            final = 'ㅇㅑ'\n",
    "            if endmark !=-1: # 문장에 __+__가 있었으면\n",
    "                sentence = attach_endmark(sentence)\n",
    "            converted = treatSF(sentence, final)\n",
    "            return converted\n",
    "        # 'ㄹ'규칙 활용\n",
    "        elif predicate in EF_R_rule:\n",
    "            result= EF_R_rule[predicate]\n",
    "            ##'아' 또는 '어' 로 처리\n",
    "            end_EF = convertSpecialCase_AhOh(sentence)\n",
    "            final = result +end_EF\n",
    "        #'르' 불규칙 활용\n",
    "        elif predicate =='ㄹㅡ':\n",
    "            # 용언 종성에 ㄹ이 있다면\n",
    "            predicate = predicate.replace('ㄹㅡ','')\n",
    "            sentence = stem+predicate + pun\n",
    "            end_EF = convertSpecialCase_AhOh(sentence)\n",
    "            end_EF = end_EF[-1]\n",
    "\n",
    "            if sentence[-1].find('ㄹ') != -1:\n",
    "                final = 'ㄹ'+end_EF\n",
    "            else:\n",
    "                final  = 'ㄹㄹ'+end_EF\n",
    "\n",
    "        ##'우' 불규칙 활용\n",
    "        # '푸'를 제외한 다른 'ㅜ'는 'ㅓ'와 결합\n",
    "        elif predicate.find('ㅍㅜ') !=-1:\n",
    "            predicate = predicate.replace('ㅜ','')\n",
    "            sentence = stem+predicate + pun\n",
    "            final = 'ㅓ'\n",
    "        #'오' 불규칙 활용(현 버전에서는 고려하지 않음. It is not considered in the current version.)\n",
    "        #'하' 불규칙 활용\n",
    "        elif predicate.find('ㅎㅏ') !=-1:\n",
    "            predicate = predicate.replace('ㅏ','')\n",
    "            sentence = stem + predicate + pun\n",
    "            final = 'ㅐ'\n",
    "        #활용이 안되었던 용언 처리\n",
    "        else:\n",
    "            if predicate.find('ㅡ') !=-1:\n",
    "                end_EF = convertSpecialCase_AhOh(sentence)\n",
    "                predicate = predicate.replace('ㅡ','')\n",
    "                sentence = stem + predicate + pun\n",
    "                final = end_EF[-1]\n",
    "            ## 수정할 필요 있음. (In the current version, there is a possibility of an error in the following part.)\n",
    "            elif predicate.find('ㅗ') !=-1:\n",
    "                predicate = predicate.replace('ㅗ','')\n",
    "                sentence = stem+predicate+ pun\n",
    "                final = 'ㅘ'\n",
    "            elif predicate.find('ㅜ') !=-1:\n",
    "                predicate = predicate.replace('ㅜ','') \n",
    "                sentence = stem + predicate+ pun\n",
    "                final = 'ㅝ'\n",
    "            elif predicate.find('ㅣ') !=-1:\n",
    "                predicate = predicate.replace('ㅣ','')\n",
    "                sentence = stem+predicate+ pun\n",
    "                final = 'ㅕ'\n",
    "            else:\n",
    "                final = convertSpecialCase_AhOh(sentence)\n",
    "                if  predicate.find('ㅏ') !=-1:\n",
    "                    final = ''\n",
    "                elif  predicate.find('ㅓ') !=-1:\n",
    "                    final = check_NoRule(stem,predicate)\n",
    "                    if final != '':\n",
    "                        ind = sentence.rfind('ㅓ')\n",
    "                        sentence = sentence[:ind]+sentence[ind+1:]\n",
    "                elif  predicate.find('ㅐ') !=-1:\n",
    "                    final = ''\n",
    "    #                 return final, sentence\n",
    "    #             return final, sentence\n",
    "    #     return final, sentence\n",
    "        if endmark !=-1: # 문장에 __+__가 있었으면\n",
    "            sentence = attach_endmark(sentence)\n",
    "        converted = treatSF(sentence, final)\n",
    "        return converted\n",
    "\n",
    "    def __convertSpecialCase_Da(self, stc, ending, taglist):\n",
    "        sentence, endmark = detach_endmark(stc)\n",
    "        final =''\n",
    "        isEp = sentence[-2:-1]\n",
    "        if (isEp == 'ㅆ' or sentence.find('ㅇㅏㄶ') !=-1)and (taglist.find('VV/EF') !=-1 or taglist.find('VX/EF') !=-1):\n",
    "            final = 'ㄷㅏ'\n",
    "        elif taglist.find('EP/EF') !=-1 or taglist.find('VA/EF') !=-1 or taglist.find('VX/EF') !=-1:\n",
    "            final = 'ㄷㅏ'\n",
    "        elif taglist.find('VV/EF') !=-1:\n",
    "            final = 'ㄴㅡㄴㄷㅏ'\n",
    "        elif taglist.find('VV') !=-1:\n",
    "            final = 'ㄴㅡㄴㄷㅏ'\n",
    "        if endmark !=-1: # 문장에 __+__가 있었으면\n",
    "            sentence = attach_endmark(sentence)\n",
    "        converted = treatSF(sentence, final)\n",
    "        return converted\n",
    "\n",
    "    def __convertSpecialCase_Nida(self, sentence, ending, taglist):\n",
    "        #시가 있으면 ㄴ다를 붙인다. \n",
    "        #형용사, 서술격 조사일 경우 convertSpecialCase_SaeYo를 통해 변경한 다음, 아/어를 제거 후 다를 붙이고\n",
    "        #동사일 경우 ㄴ다를 붙여서 해결한다. \n",
    "        predicate = check_VV_VA(sentence, taglist)\n",
    "        #VV\n",
    "        if predicate == 1:\n",
    "            final = 'ㄴㄷㅏ'\n",
    "            converted = treatSF(sentence, final)\n",
    "        #VA\n",
    "        #형용사의 경우, 세요를 거친 후 마지막을 붙인다면 이상해질 수 있다. 그냥 khaiii를 쓰는 것이 안전하다고 판단된다. \n",
    "        elif predicate == 0:\n",
    "            final = 'ㄷㅏ'\n",
    "            temp=treatSF(sentence,ending)\n",
    "            jam1 = Jamodealer(temp)\n",
    "            s = jam1.make_one()\n",
    "            converted_kh = proc_khaiii(s)\n",
    "            converted_kh = converted_kh.replace('ㅂ니다', '__+__')\n",
    "            converted = treatSF(converted_kh, final)\n",
    "        else:\n",
    "            final = 'ㄷㅏ'\n",
    "            converted = treatSF(sentence, final)\n",
    "        return converted\n",
    "    def __convertSpecialCase_Yo(self, stc, ending, tag):\n",
    "        sentence, endmark = detach_endmark(stc)\n",
    "        pun = sentence[-1:]\n",
    "        predicate = sentence[-3:-1]\n",
    "        stem = sentence[:-3]\n",
    "        isVcp = tag[-11:]\n",
    "\n",
    "        temp =''\n",
    "        ni = 0\n",
    "        after_si, ni = delete_EP_si(sentence, tag)\n",
    "        if after_si != '':\n",
    "            sentence = after_si\n",
    "            after_si =''\n",
    "\n",
    "        if isVcp.find('VCP') != -1:\n",
    "            final = 'ㅇㅑ'\n",
    "        else:\n",
    "            final = ''\n",
    "\n",
    "        if endmark !=-1: # 문장에 __+__가 있었으면\n",
    "            sentence = attach_endmark(sentence)\n",
    "\n",
    "        converted = treatSF(sentence, final)\n",
    "        return converted\n",
    "    def __convertSpecialCase_NaYo(self, sentence, ending, taglist):\n",
    "        final ='ㄴㅣ'\n",
    "        if taglist.find('/VCP+EF/') !=-1:\n",
    "            final='ㄴㅏㄴㅣ'\n",
    "        converted = treatSF(sentence, final)\n",
    "        return converted\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def processText(self,stc):\n",
    "        result = stc\n",
    "        flag = 0\n",
    "        if result[-1]=='\\n':\n",
    "            result = result.replace('\\n','')   \n",
    "        num = 0\n",
    "        while 1:\n",
    "            if result[-1-num]!=' ':\n",
    "                break\n",
    "            else:\n",
    "                num = num+1\n",
    "                \n",
    "        if num==0:\n",
    "            rere = result\n",
    "        else:\n",
    "            rere = result[:-num]\n",
    "            \n",
    "        \n",
    "        r_pun = ''\n",
    "        r_word = rere\n",
    "        while True:\n",
    "            if r_word[-1] in SV_LIST:\n",
    "                r_pun = r_pun+r_word[-1]\n",
    "                r_word = r_word[:-1]\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        num_space = 0\n",
    "        for i in r_word:\n",
    "            if i==' ':\n",
    "                num_space = num_space+1\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        if num_space!=0:\n",
    "            r_word = r_word[num_space:]\n",
    "\n",
    "        plus = ''\n",
    "        for s in range(num_space):\n",
    "            plus = plus+' '\n",
    "    \n",
    "        if r_word[-1] =='?' or r_word[-1] =='.' or r_word[-1] =='!' or r_word[-1] =='\\\"':\n",
    "            r_word = r_word\n",
    "        else:\n",
    "            r_word = r_word+'.'\n",
    "            flag = 1\n",
    "\n",
    "        res = self.to_low(r_word)\n",
    "##########For data extraction##########\n",
    "#         try:\n",
    "#             res = self.to_low(r_word)\n",
    "#         except:\n",
    "#             print('exception sentece number: ', count)\n",
    "#             res = r_word\n",
    "##########For data extraction##########\n",
    "\n",
    "        r_word = plus+r_word\n",
    "        res = plus+res\n",
    "        \n",
    "        if flag ==1:\n",
    "            res = res[:-1]\n",
    "\n",
    "        return res+r_pun[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Korean Sentence:  누리호가 발사를 성공했어요!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Result: 누리호가 발사를 성공했어!\n"
     ]
    }
   ],
   "source": [
    "txt = input(\"Enter Korean Sentence: \")\n",
    "ch = Changer_low()\n",
    "output = ch.processText(txt)\n",
    "print(\"Converted Result:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
